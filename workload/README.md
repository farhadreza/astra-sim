This layer sits on top of system layer and implements the different training loops and parallelization strategies.
It talks to the system layer using collective primitives provided by system layer.
